{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madhavjk/AI/blob/main/Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTWdmhEEDD4-"
      },
      "source": [
        "# Building a simple classifier on top of BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgJN94CzH2u6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "866a96cd-85b3-4d14-be5e-3b7a97158ad5"
      },
      "source": [
        "# We'll use Huggingface's Transformers package\n",
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import BertPreTrainedModel, BertTokenizer, BertModel, BertConfig\n",
        "from transformers.modeling_bert import BertPooler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.38)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.38)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcixEYV70qOi"
      },
      "source": [
        "Prepare a dataset of sentences related to COVID-19, and sentences about BERT. By leveraging the information contained in the pretrained model, we're going to train a model that has a strong ability to generalize beyond the classification training examples. We put the data in a Pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv5JzIaTH-3M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "54bf4d6b-26ec-401c-c831-cc230ea03605"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "covid_texts = [\n",
        "  \"Approximately 90 days of the SARS-CoV-2 (COVID-19) spreading originally from Wuhan, China, and across the globe has led to a widespread chain of events with imminent threats to the fragile relationship between community health and economic health.\",\n",
        "  \"Despite near hourly reporting on this crisis, there has been no regular, updated, or accurate reporting of hospitalizations for COVID-19.\",\n",
        "  \"It is known that many test-positive individuals may not develop symptoms or have a mild self-limited viral syndrome consisting of fever, malaise, dry cough, and constitutional symptoms.\",\n",
        "  \"However some individuals develop a more fulminant syndrome including viral pneumonia, respiratory failure requiring oxygen, acute respiratory distress syndrome requiring mechanical ventilation, and in substantial fractions leading to death attributable to COVID-19.\",\n",
        "  \"The pandemic is evolving in a clustered, non-inform fashion resulting in many hospitals with preparedness but few or no cases, and others that are completely overwhelmed.\",\n",
        "  \"Thus, a considerable risk of spread when personal protection equipment becomes exhausted and a large fraction of mortality in those not offered mechanical ventilation are both attributable to a crisis due to maldistribution of resources.\",\n",
        "  \"The pandemic is amenable to self-reporting through a mobile phone application that could obtain critical information on suspected cases and report on the results of self testing and actions taken.\",\n",
        "  \"The only method to understand the clustering and the immediate hospital resource needs is mandatory, uniform, daily reporting of hospital censuses of COVID-19 cases admitted to hospital wards and intensive care units.\",\n",
        "  \"Current reports of hospitalizations are delayed, uncertain, and wholly inadequate.\",\n",
        "  \"This paper urges all the relevant stakeholders to take up self-reporting and reporting of hospitalizations of COVID-19 as an urgent task in combating this devastating pandemic.\"]\n",
        "bert_texts = [\n",
        "  \"Bidirectional Encoder Representations from Transformers (BERT) is a technique for NLP (Natural Language Processing) pre-training developed by Google.\",\n",
        "  \"BERT was created and published in 2018 by Jacob Devlin and his colleagues from Google.\",\n",
        "  \"Google is leveraging BERT to better understand user searches.\",\n",
        "  \"The reasons for BERT's state-of-the-art performance on these natural language understanding tasks are not yet well understood.\",\n",
        "  \"Current research has focused on investigating the relationship behind BERT's output as a result of carefully chosen input sequences,[6][7] analysis of internal vector representations through probing classifiers,[8][9] and the relationships represented by attention weights.\",                  \n",
        "  \"BERT has its origins from pre-training contextual representations including Semi-supervised Sequence Learning,[10] Generative Pre-Training, ELMo,[11] and ULMFit.[12]\",\n",
        "  \"Unlike previous models, BERT is a deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus.\",\n",
        "  \"Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary, where BERT is deeply bidirectional.\",\n",
        "  \"On October 25, 2019, Google Search announced that they had started applying BERT models for English language search queries within the US.[13]\",\n",
        "  \"On December 9, 2019, it was reported that BERT had been adopted by Google Search for over 70 languages.[14]\"\n",
        "]\n",
        "\n",
        "raw_data = pd.DataFrame({\"text\": covid_texts + bert_texts,\n",
        "                      \"label\": [\"COVID-19\"]*len(covid_texts) + [\"machine-learning\"]*len(bert_texts)})\n",
        "raw_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Approximately 90 days of the SARS-CoV-2 (COVID...</td>\n",
              "      <td>COVID-19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Despite near hourly reporting on this crisis, ...</td>\n",
              "      <td>COVID-19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It is known that many test-positive individual...</td>\n",
              "      <td>COVID-19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>However some individuals develop a more fulmin...</td>\n",
              "      <td>COVID-19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The pandemic is evolving in a clustered, non-i...</td>\n",
              "      <td>COVID-19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Thus, a considerable risk of spread when perso...</td>\n",
              "      <td>COVID-19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>The pandemic is amenable to self-reporting thr...</td>\n",
              "      <td>COVID-19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>The only method to understand the clustering a...</td>\n",
              "      <td>COVID-19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Current reports of hospitalizations are delaye...</td>\n",
              "      <td>COVID-19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>This paper urges all the relevant stakeholders...</td>\n",
              "      <td>COVID-19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Bidirectional Encoder Representations from Tra...</td>\n",
              "      <td>machine-learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>BERT was created and published in 2018 by Jaco...</td>\n",
              "      <td>machine-learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Google is leveraging BERT to better understand...</td>\n",
              "      <td>machine-learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>The reasons for BERT's state-of-the-art perfor...</td>\n",
              "      <td>machine-learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Current research has focused on investigating ...</td>\n",
              "      <td>machine-learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>BERT has its origins from pre-training context...</td>\n",
              "      <td>machine-learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Unlike previous models, BERT is a deeply bidir...</td>\n",
              "      <td>machine-learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Context-free models such as word2vec or GloVe ...</td>\n",
              "      <td>machine-learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>On October 25, 2019, Google Search announced t...</td>\n",
              "      <td>machine-learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>On December 9, 2019, it was reported that BERT...</td>\n",
              "      <td>machine-learning</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 text             label\n",
              "0   Approximately 90 days of the SARS-CoV-2 (COVID...          COVID-19\n",
              "1   Despite near hourly reporting on this crisis, ...          COVID-19\n",
              "2   It is known that many test-positive individual...          COVID-19\n",
              "3   However some individuals develop a more fulmin...          COVID-19\n",
              "4   The pandemic is evolving in a clustered, non-i...          COVID-19\n",
              "5   Thus, a considerable risk of spread when perso...          COVID-19\n",
              "6   The pandemic is amenable to self-reporting thr...          COVID-19\n",
              "7   The only method to understand the clustering a...          COVID-19\n",
              "8   Current reports of hospitalizations are delaye...          COVID-19\n",
              "9   This paper urges all the relevant stakeholders...          COVID-19\n",
              "10  Bidirectional Encoder Representations from Tra...  machine-learning\n",
              "11  BERT was created and published in 2018 by Jaco...  machine-learning\n",
              "12  Google is leveraging BERT to better understand...  machine-learning\n",
              "13  The reasons for BERT's state-of-the-art perfor...  machine-learning\n",
              "14  Current research has focused on investigating ...  machine-learning\n",
              "15  BERT has its origins from pre-training context...  machine-learning\n",
              "16  Unlike previous models, BERT is a deeply bidir...  machine-learning\n",
              "17  Context-free models such as word2vec or GloVe ...  machine-learning\n",
              "18  On October 25, 2019, Google Search announced t...  machine-learning\n",
              "19  On December 9, 2019, it was reported that BERT...  machine-learning"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y0y8kobDHO2"
      },
      "source": [
        "Create a Torch [map-style dataset](https://pytorch.org/docs/stable/data.html#map-style-datasets) that processes our dataframe using the appropriate BERT tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1IO_SL_Iexa"
      },
      "source": [
        "class ClassificationDataset(Dataset):\n",
        "  def __init__(self, dataframe, tokenizer):\n",
        "    super(ClassificationDataset, self).__init__()\n",
        "    self.data = dataframe\n",
        "    self.label_mapping = {label: i for i, label in enumerate(set(dataframe[\"label\"]))}\n",
        "    self.id2label = {i: label for i, label in enumerate(set(dataframe[\"label\"]))}\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "    # Determine the maximum length of a sequence in the training data.\n",
        "    # This approach does involve encoding the dataset twice which could be avoided if necessary.\n",
        "    encoded_texts = [tokenizer.encode(text) for text in dataframe[\"text\"]]\n",
        "    max_len = max([len(text) for text in encoded_texts])\n",
        "    self.encoded_seqs = [tokenizer.encode_plus(text, pad_to_max_length=True, max_length=max_len)\n",
        "                         for text in dataframe[\"text\"]]\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    seq = self.encoded_seqs[index]\n",
        "    return {\"text\": torch.tensor(seq[\"input_ids\"]),\n",
        "            \"attention_mask\": torch.tensor(seq[\"attention_mask\"]),\n",
        "            \"label\": torch.tensor(self.label_mapping[self.data.loc[index].label])}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfYVJRZF49IT"
      },
      "source": [
        "Here we create our own custom \"head\" that wraps the model.\n",
        "\n",
        "It's fine to use BertForSequenceClassification here instead. (It would actually be good to do a side-by-side comparison of the convergence of learning for this randomly initialized classifier and using BertForSequenceClassification; maybe there's no big diff)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXVFrlGLHmkz"
      },
      "source": [
        "class BertClassifier(BertPreTrainedModel):\n",
        "    \"\"\" A modification of BertForSequenceClassification which doesn't use the parameters trained on NSP,\n",
        "    instead re-initializing params.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n",
        "        self.pooler = BertPooler(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        ):\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "        hiddens = outputs[0]\n",
        "        pooled_output = self.pooler(hiddens)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = nn.CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGxzuCSOX1mP"
      },
      "source": [
        "Best practices involve shuffling our examples here. But we leave that as an exercise for the reader.\n",
        "\n",
        "We'd also run this on a GPU in the real world."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIB1Gr1FHxXW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "98009948-ee0b-42b8-e136-e89fe67f219a"
      },
      "source": [
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertClassifier.from_pretrained(model_name)\n",
        "config = BertConfig.from_pretrained(model_name)\n",
        "\n",
        "torch_dataset = ClassificationDataset(raw_data, tokenizer)\n",
        "torch_dataloader = DataLoader(torch_dataset, batch_size=4) #Hint: Shuffle most easily applied here\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(10):\n",
        "    print(\"epoch: {}\".format(epoch))\n",
        "    for batch in torch_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        loss, logits =  model(batch[\"text\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"label\"])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(\"loss: {}\".format(loss.item()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0\n",
            "loss: 1.374000072479248\n",
            "epoch: 1\n",
            "loss: 0.32587191462516785\n",
            "epoch: 2\n",
            "loss: 0.06642933189868927\n",
            "epoch: 3\n",
            "loss: 0.025280989706516266\n",
            "epoch: 4\n",
            "loss: 0.012373460456728935\n",
            "epoch: 5\n",
            "loss: 0.007129683159291744\n",
            "epoch: 6\n",
            "loss: 0.006250674836337566\n",
            "epoch: 7\n",
            "loss: 0.004350499715656042\n",
            "epoch: 8\n",
            "loss: 0.0033491668291389942\n",
            "epoch: 9\n",
            "loss: 0.0033215307630598545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tsb3BK2X5pPv"
      },
      "source": [
        "# Applying the trained model to sentences with words not seen in classification training\n",
        "\n",
        "Now let's create some novel sentences that don't use vocabulary from the training set!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tlZedbqH0mO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4472c1a8-1baa-457d-b6a0-0d333d9938b5"
      },
      "source": [
        "all_covid = \" \".join(covid_texts)\n",
        "all_covid"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Approximately 90 days of the SARS-CoV-2 (COVID-19) spreading originally from Wuhan, China, and across the globe has led to a widespread chain of events with imminent threats to the fragile relationship between community health and economic health. Despite near hourly reporting on this crisis, there has been no regular, updated, or accurate reporting of hospitalizations for COVID-19. It is known that many test-positive individuals may not develop symptoms or have a mild self-limited viral syndrome consisting of fever, malaise, dry cough, and constitutional symptoms. However some individuals develop a more fulminant syndrome including viral pneumonia, respiratory failure requiring oxygen, acute respiratory distress syndrome requiring mechanical ventilation, and in substantial fractions leading to death attributable to COVID-19. The pandemic is evolving in a clustered, non-inform fashion resulting in many hospitals with preparedness but few or no cases, and others that are completely overwhelmed. Thus, a considerable risk of spread when personal protection equipment becomes exhausted and a large fraction of mortality in those not offered mechanical ventilation are both attributable to a crisis due to maldistribution of resources. The pandemic is amenable to self-reporting through a mobile phone application that could obtain critical information on suspected cases and report on the results of self testing and actions taken. The only method to understand the clustering and the immediate hospital resource needs is mandatory, uniform, daily reporting of hospital censuses of COVID-19 cases admitted to hospital wards and intensive care units. Current reports of hospitalizations are delayed, uncertain, and wholly inadequate. This paper urges all the relevant stakeholders to take up self-reporting and reporting of hospitalizations of COVID-19 as an urgent task in combating this devastating pandemic.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbb9GsK3c5eX"
      },
      "source": [
        "Do any salient words in this sentence occur in the COVID-19 classification data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2AhmIlYYckx"
      },
      "source": [
        "covid_test = \"coronaviruses afflict the lungs and can be deadly\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zNGdxArYeE5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "a9f86e5b-4b58-490a-8153-e2ca3c1f2c30"
      },
      "source": [
        "fmt = \"{: <14}{}\"\n",
        "print(fmt.format(\"Word\", \"In train?\"))\n",
        "print(\"-----------------------\")\n",
        "for tok in covid_test.split():\n",
        "  print(fmt.format(tok, tok in all_covid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word          In train?\n",
            "-----------------------\n",
            "coronaviruses False\n",
            "afflict       False\n",
            "the           True\n",
            "lungs         False\n",
            "and           True\n",
            "can           False\n",
            "be            True\n",
            "deadly        False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgmsGK_q6jJe"
      },
      "source": [
        "The only words from this sentence seen in the COVID-19 training data are stopwords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3Df6LLP6r3L"
      },
      "source": [
        "Now let's create a sentence about machine learning that doesn't contain any relevant words from the classification training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_GhuT1SZ6Pd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "6b9e9208-99c6-46a5-9e6a-03cc4aa2713e"
      },
      "source": [
        "ml_test = \"Engineers at Amazon have also made numerous automatic speech recognition advances\"\n",
        "all_bert = \" \".join(bert_texts)\n",
        "print(fmt.format(\"Word\", \"In train?\"))\n",
        "print(\"-----------------------\")\n",
        "for tok in ml_test.split():\n",
        "  print(fmt.format(tok, tok in all_bert))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word          In train?\n",
            "-----------------------\n",
            "Engineers     False\n",
            "at            True\n",
            "Amazon        False\n",
            "have          False\n",
            "also          False\n",
            "made          False\n",
            "numerous      False\n",
            "automatic     False\n",
            "speech        False\n",
            "recognition   False\n",
            "advances      False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtNwIr1CKdtt"
      },
      "source": [
        "Let's run these through the classifier and see what we get."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWA9ZCrmYpVH"
      },
      "source": [
        "test_dataframe = pd.DataFrame({\"text\": [covid_test, ml_test],\n",
        "                               \"label\": [\"COVID-19\", \"machine-learning\"]})\n",
        "test_dataset = ClassificationDataset(test_dataframe, tokenizer)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxNiGVNlZRpc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e1fe3c5e-8370-4bab-a0fa-06b42a6da4f6"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    logits, *_ =  model(batch[\"text\"], attention_mask=batch[\"attention_mask\"])\n",
        "    print(torch.softmax(logits, dim=1))\n",
        "    for seq_logits in logits:\n",
        "      label_id = torch.argmax(seq_logits)\n",
        "      print(test_dataset.id2label[label_id.item()])\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0040, 0.9960],\n",
            "        [0.9963, 0.0037]], grad_fn=<SoftmaxBackward>)\n",
            "COVID-19\n",
            "machine-learning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3SzwnVST8ef"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}