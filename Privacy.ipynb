{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Privacy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madhavjk/AI/blob/main/Privacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vls8FTYd_koz"
      },
      "source": [
        "# Privacy issue: models store information from the training data\n",
        "\n",
        "In this we demonstrate how the predictive capacity of pre-trained transformers can reveal information about the data the model was trained on. The ability of neural networks to memorize training data is something to bear in mind when training models on sensitive data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k50L2rVhwugz"
      },
      "source": [
        "# We'll use Huggingface's Transformers package\n",
        "!pip install transformers\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import BertForPreTraining, BertTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyhlh_rwAwLh"
      },
      "source": [
        "The BertForPreTraining subclass is used for the masked language model pre-training. That task involves predicting missing words. We'll use this to extract information from the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vtak0rvvw3lx"
      },
      "source": [
        "# Load a base BERT model\n",
        "model_name = \"bert-base-uncased\"\n",
        "model = BertForPreTraining.from_pretrained(model_name)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrL9DyrpBP2x"
      },
      "source": [
        "Define a function that takes the start of a sentence and the end of a sentence and predicts a single token in between."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3RMY75V4BSf"
      },
      "source": [
        "def predict_mask(prefix, suffix):\n",
        "  tokens = [tokenizer.cls_token] + tokenizer.tokenize(prefix) + [tokenizer.mask_token] + tokenizer.tokenize(suffix) + [tokenizer.sep_token]\n",
        "  mask_loc = tokens.index(tokenizer.mask_token)\n",
        "  token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  tensor = torch.tensor([token_ids])\n",
        "  pred_scores, _seq_rel_scores = model(input_ids=tensor)\n",
        "  mask_logits = pred_scores[0, mask_loc, :]\n",
        "  mask_word_pred = torch.argmax(mask_logits)\n",
        "  return \"{} **{}** {}\".format(prefix, tokenizer.convert_ids_to_tokens([mask_word_pred])[0], suffix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERblBX44Bb9t"
      },
      "source": [
        "Create a set of probe sentences designed to extract information from the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU0BNo6Z46h9"
      },
      "source": [
        "probes = [\n",
        "  (\"Berlin is the\", \"of Germany\"),\n",
        "  (\"Marie Curie won the Nobel prize in\", \".\"),\n",
        "  (\"Bertrand\", \"was a logician, mathematician.\"),\n",
        "  (\"Bryan Wilkinson's social security number is\", \". Thankfully!\"),\n",
        "  (\"Gary Kasparov is a\", \"master.\"),\n",
        "]\n",
        "for prefix, suffix in probes:\n",
        "  print(predict_mask(prefix, suffix))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK0sKJ-2Bh_z"
      },
      "source": [
        "This BERT model was trained on Wikipedia data, so asking questions about that dataset gets sensible answers. What if you pre-trained BERT on private company data?"
      ]
    }
  ]
}